{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Regression and Classification\n",
    "\n",
    "### By Jakub Tomkiewicz and Konrad Piotrowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "The main goal of this task was to write a program that predicts whether a packet is a DDoS attack or a normal packet. We used the NSL-KDD data set, a well-known benchmark for IDS (Intrusion Detection Systems). In our report, we are comparing three different methods (Logistic Regression, Random Forest, and Decision Tree) that are built into pythons' sklearn (scikit-learn) library. The data we work on are two sets, 42 columns each. to use them for machine learning, we need to prepare them properly.\n",
    "\n",
    "Intrusion Detection Systems can \"predict\" whether a packet is malicious by, for example, comparing the packet with known attacks or looking for divergences from normal packets. Such system needs to have a high precision rate, to be able to make as little mistakes as possible. That can be achieved by training such systems on datasets such as the NSL-KDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Our motivation is the desire to see how specific machine learning methods work and how they would turn out in the direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & analysis\n",
    "\n",
    "In the beginning, we are loading the data into the program. The dataset we worked on contained two possible file types: txt and arff. We used arff because the file already included the column names. Moreover, arff file class 'column' consists of only two values (normal and anomaly), which is helpful later in the preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "train_set = arff.loadarff('KDDTrain+.arff')[0]\n",
    "test_set = arff.loadarff('KDDTest+.arff')[0]\n",
    "\n",
    "train_set = pd.DataFrame(train_set)\n",
    "test_set = pd.DataFrame(test_set)\n",
    "\n",
    "print(f'train_set dimensions: {train_set.shape}')\n",
    "print(f'test_set dimensions: {test_set.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the 'class' column consists of only two options: normal and anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_set class distribution:')\n",
    "print(train_set['class'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('test_set class distribution:')\n",
    "print(test_set['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap, we can see that many columns have a high correlation value. To make the computing easier, we can remove the columns that have a close to 1 correlation value since they will not affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_set = train_set[[column for column in train_set if train_set[column].nunique() > 1]]\n",
    "\n",
    "corr = train_set.corr()\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "train_set = train_set.drop('num_root', axis = 1)\n",
    "train_set = train_set.drop('srv_serror_rate', axis = 1)\n",
    "train_set = train_set.drop('srv_rerror_rate', axis = 1)\n",
    "train_set = train_set.drop('dst_host_srv_serror_rate', axis = 1)\n",
    "train_set = train_set.drop('dst_host_serror_rate', axis = 1)\n",
    "train_set = train_set.drop('dst_host_rerror_rate', axis = 1)\n",
    "train_set = train_set.drop('dst_host_same_srv_rate', axis = 1)\n",
    "train_set = train_set.drop('dst_host_srv_rerror_rate', axis = 1)\n",
    "\n",
    "test_set = test_set.drop('num_root', axis = 1)\n",
    "test_set = test_set.drop('srv_serror_rate', axis = 1)\n",
    "test_set = test_set.drop('srv_rerror_rate', axis = 1)\n",
    "test_set = test_set.drop('dst_host_srv_serror_rate', axis = 1)\n",
    "test_set = test_set.drop('dst_host_serror_rate', axis = 1)\n",
    "test_set = test_set.drop('dst_host_rerror_rate', axis = 1)\n",
    "test_set = test_set.drop('dst_host_srv_rerror_rate', axis = 1)\n",
    "test_set = test_set.drop('dst_host_same_srv_rate', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need only numerical values in our dataset to perform the calculations, we must somehow change all strings into these numerical substitutes. We have created dictionaries with all unique strings for four columns (protocol_type, flag, service, class) and mapped the string values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "protocol_types_dictionary = {}\n",
    "\n",
    "types = test_set['protocol_type'].unique()\n",
    "for x in range(len(types)):\n",
    "    protocol_types_dictionary[types[x]] = x\n",
    "test_set['protocol_type'] = test_set['protocol_type'].map(protocol_types_dictionary)\n",
    "train_set['protocol_type'] = train_set['protocol_type'].map(protocol_types_dictionary)\n",
    "\n",
    "flag_types_dictionary = {}\n",
    "\n",
    "types = test_set['flag'].unique()\n",
    "for x in range(len(types)):\n",
    "    flag_types_dictionary[types[x]] = x\n",
    "test_set['flag'] = test_set['flag'].map(flag_types_dictionary)\n",
    "train_set['flag'] = train_set['flag'].map(flag_types_dictionary)\n",
    "\n",
    "service_types_dictionary = {}\n",
    "\n",
    "types = train_set['service'].unique()\n",
    "for x in range(len(types)):\n",
    "    service_types_dictionary[types[x]] = x\n",
    "train_set['service'] = train_set['service'].map(service_types_dictionary)\n",
    "test_set['service'] = test_set['service'].map(service_types_dictionary)\n",
    "\n",
    "class_types_dictionary = {}\n",
    "\n",
    "types = test_set['class'].unique()\n",
    "for x in range(len(types)):\n",
    "    class_types_dictionary[types[x]] = x\n",
    "train_set['class'] = train_set['class'].map(class_types_dictionary)\n",
    "test_set['class'] = test_set['class'].map(class_types_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of columns in the train and test sets were different, we needed to check the difference in the collections of column names for both data sets. It turned out that the test set contained an additional row named num_outbound_cmds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "train_set_columns = train_set.columns\n",
    "test_set_columns = test_set.columns\n",
    "\n",
    "difference = test_set_columns.difference(train_set_columns)\n",
    "\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.drop('num_outbound_cmds', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Y values, we need to set the class columns since they contain the information on whether the packet is an anomaly or not. We can drop the class column when the Y is set and place all the remaining columns' data as the X value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "train_y = train_set['class']\n",
    "\n",
    "train_set = train_set.drop('class', axis = 1)\n",
    "\n",
    "train_x = train_set.iloc[:, :].values\n",
    "\n",
    "test_y = test_set['class']\n",
    "\n",
    "test_set = test_set.drop('class', axis = 1)\n",
    "\n",
    "test_x = test_set.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the pythons' sklearn (scikit-learn) library, the prediction and evaluation process is pretty straightforward. \n",
    "\n",
    "We used three different methods: Logistic Regression, Random Forest, and Decision Tree. \n",
    "\n",
    "In all of them, the implementation is nearly the same. We train the model using a suitable method that takes the input and output of the train data set as the parameters. After that, we can predict the test output value using predict method. The last step is to calculate the accuracy using the scoring method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The first method that we decided to use is logistic regression. It is a method for statistical analysis that predicts binary outcomes. In some cases, that may not be sufficient, but a binary result is enough since we need to decide if the packet is normal or an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "logistic_regression.fit(train_x, train_y)\n",
    "\n",
    "lr_train_time = time.time() - start_time\n",
    "\n",
    "print(f'training time: {lr_train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "logistic_regression.predict(test_x)\n",
    "\n",
    "lr_test_time = time.time() - start_time\n",
    "\n",
    "print(f'testing time: {lr_test_time}')\n",
    "\n",
    "lr_score = logistic_regression.score(test_x, test_y)\n",
    "print(f'accuracy: {lr_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest works by creating a set of decision trees, and combining the outcome of all of them into a single result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_forest.fit(train_x, train_y)\n",
    "\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "print(f'training time: {rf_train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_forest.predict(test_x)\n",
    "\n",
    "rf_test_time = time.time() - start_time\n",
    "\n",
    "print(f'testing time: {rf_test_time}')\n",
    "\n",
    "rf_score = random_forest.score(test_x, test_y)\n",
    "print(f'accuracy: {rf_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "decision_tree.fit(train_x, train_y)\n",
    "\n",
    "dt_train_time = time.time() - start_time\n",
    "\n",
    "print(f'training time: {dt_train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "decision_tree.predict(test_x)\n",
    "\n",
    "dt_test_time = time.time() - start_time\n",
    "\n",
    "print(f'testing time: {dt_test_time}')\n",
    "\n",
    "dt_score = decision_tree.score(test_x, test_y)\n",
    "print(f'accuracy: {dt_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the accuracy of the three methods, the apparent differences are tiny. the best is Random Forest, but Logistic Regression and Decision Tree are only marginally worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Random Forest', 'Decision Tree']\n",
    "\n",
    "scores = [lr_score, rf_score, dt_score]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot()\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Accuracy comparison')\n",
    "plt.bar(names, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing training time gives the most spectacular differences. Decision Tree is by far the fastest, almost five times faster than Logistic Regression, not even comparable with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Random Forest', 'Decision Tree']\n",
    "\n",
    "scores = [lr_train_time, rf_train_time, dt_train_time]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot()\n",
    "plt.ylim([0, 8])\n",
    "plt.title('Train time comparison')\n",
    "plt.bar(names, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the testing time, it's hard to find a favorite anymore. Logistic Regression and Decision Tree are practically equal and, therefore, much faster than Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Random Forest', 'Decision Tree']\n",
    "\n",
    "scores = [lr_test_time, rf_test_time, dt_test_time]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot()\n",
    "plt.ylim([0, 0.3])\n",
    "plt.title('Test time comparison')\n",
    "plt.bar(names, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up the comparison of both times and accuracy, we can say that the Random Forest is the most accurate but the slowest. However, Decision Tree is by far the fastest, and its accuracy is only slightly inferior to that of the Random Forest."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5747abed4665cd73bfee1b4c496f051b35d91587297a2f4627dce69f61dc3a6d"
  },
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
